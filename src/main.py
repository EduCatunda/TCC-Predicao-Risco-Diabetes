# -*- coding: utf-8 -*-
"""Cópia de Genetic Algorithm for Feature Selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R6eNOdd1-tchPDeCdov3StegK7pNLbhh

#Importing the required libraries

##1. Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from random import randint
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay
from sklearn.model_selection import KFold, cross_val_score
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import StratifiedKFold

plt.style.use('ggplot')

"""##2. Classificadores"""

class ELMClassifier:
    def __init__(self, n_hidden):
        self.n_hidden = n_hidden

    def _sigmoid(self, x):
        return 1.0 / (1.0 + np.exp(-x))

    def fit(self, X, y):
        n_samples, n_features = X.shape

        # Generate random weights and biases
        self.input_weights = np.random.normal(size=(n_features, self.n_hidden))
        self.bias = np.random.normal(size=(1, self.n_hidden))

        # Calculate hidden layer output
        hidden_output = self._sigmoid(np.dot(X, self.input_weights) + self.bias)

        # Pseudo-inverse for output weights
        self.output_weights = np.dot(np.linalg.pinv(hidden_output), y)

    def predict(self, X):
        hidden_output = self._sigmoid(np.dot(X, self.input_weights) + self.bias)
        y_pred = np.dot(hidden_output, self.output_weights)
        return np.round(y_pred)

classifiers = ['RandomForest','DecisionTree',
               'GradientBoosting',
               'ELM',       'MLP',
               'LinearSVM', 'RadialSVM(RBF)']


models = [RandomForestClassifier(n_estimators=200, random_state=0),
          DecisionTreeClassifier(random_state=0),
          GradientBoostingClassifier(random_state=0),
          ELMClassifier(n_hidden=86),
          MLPClassifier(),
          SVC(kernel='linear'),
          SVC(kernel='rbf')]

def split(df, label):
    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)
    return X_tr, X_te, Y_tr, Y_te

def acc_score(df, label):
    Score = pd.DataFrame({"Classifier":classifiers})
    j = 0
    acc = []
    X_train,X_test,Y_train,Y_test = split(df,label)
    for i in models:
        model = i
        model.fit(X_train,Y_train)
        predictions = model.predict(X_test)
        acc.append(accuracy_score(Y_test,predictions))
        j = j+1
    Score["Accuracy"] = acc
    Score.sort_values(by="Accuracy", ascending=False,inplace = True)
    Score.reset_index(drop=True, inplace=True)
    return Score

def plot(score, x, y, c = "b"):
    gen = [1,2,3,4,5]
    plt.figure(figsize=(6,4))
    ax = sns.pointplot(x=gen, y=score,color = c )
    ax.set(xlabel="Generation", ylabel="Accuracy")
    ax.set(ylim=(x,y))

"""##3. Fitness"""

def initilization_of_population(size,n_feat):
    population = []
    for i in range(size):
        chromosome = np.ones(n_feat,dtype=np.bool)
        chromosome[:int(0.3*n_feat)]=False
        np.random.shuffle(chromosome)
        population.append(chromosome)
    return population


def fitness_score(population):
    scores = []
    for chromosome in population:
        logmodel.fit(X_train.iloc[:,chromosome],Y_train)
        predictions = logmodel.predict(X_test.iloc[:,chromosome])
        scores.append(accuracy_score(Y_test,predictions))
    scores, population = np.array(scores), np.array(population)
    inds = np.argsort(scores)
    return list(scores[inds][::-1]), list(population[inds,:][::-1])


def selection(pop_after_fit,n_parents):
    population_nextgen = []
    for i in range(n_parents):
        population_nextgen.append(pop_after_fit[i])
    return population_nextgen


def crossover(pop_after_sel):
    pop_nextgen = pop_after_sel
    for i in range(0,len(pop_after_sel),2):
        new_par = []
        child_1 , child_2 = pop_nextgen[i] , pop_nextgen[i+1]
        new_par = np.concatenate((child_1[:len(child_1)//2],child_2[len(child_1)//2:]))
        pop_nextgen.append(new_par)
    return pop_nextgen


def mutation(pop_after_cross,mutation_rate,n_feat):
    mutation_range = int(mutation_rate*n_feat)
    pop_next_gen = []
    for n in range(0,len(pop_after_cross)):
        chromo = pop_after_cross[n]
        rand_posi = []
        for i in range(0,mutation_range):
            pos = randint(0,n_feat-1)
            rand_posi.append(pos)
        for j in rand_posi:
            chromo[j] = not chromo[j]
        pop_next_gen.append(chromo)
    return pop_next_gen

def generations(df,label,size,n_feat,n_parents,mutation_rate,n_gen,X_train,
                                   X_test, Y_train, Y_test):
    best_chromo= []
    best_score= []
    population_nextgen=initilization_of_population(size,n_feat)
    for i in range(n_gen):
        scores, pop_after_fit = fitness_score(population_nextgen)
        print('Best score in generation',i+1,':',scores[:1])  #2
        pop_after_sel = selection(pop_after_fit,n_parents)
        pop_after_cross = crossover(pop_after_sel)
        population_nextgen = mutation(pop_after_cross,mutation_rate,n_feat)
        best_chromo.append(pop_after_fit[0])
        best_score.append(scores[0])
    return best_chromo,best_score

"""#Early Stage Diabetes Risk Prediction

##1. Looking at dataset
"""

categorical_columns = ['Gender', 'Polyuria', 'Polydipsia', 'sudden weight loss',
                       'weakness', 'Polyphagia', 'Genital thrush', 'visual blurring', 'Itching',
                       'Irritability', 'delayed healing', 'partial paresis',
                       'muscle stiffness', 'Alopecia', 'Obesity', 'class']

data_dr = pd.read_csv('diabetes_data_upload.csv')
data_dr_binary = pd.get_dummies(data_dr, columns=categorical_columns, drop_first=True)

X = data_dr_binary.drop('class_Positive', axis=1)
y = data_dr_binary['class_Positive']

print("Early Stage Diabetes Risk Prediction Dataset:\n",X.shape[0],"Records\n",X.shape[1],"Features")

"""##2. Checking Accuracy

###Score
"""

score1 = acc_score(X, y)
score1

logmodel = ELMClassifier(n_hidden=100)
X_train, X_test, Y_train, Y_test = split(X, y)
chromo_df_dr, score_dr = generations(X, y, size=80, n_feat=X.shape[1], n_parents=64, mutation_rate=0.20, n_gen=1,
                         X_train = X_train, X_test = X_test, Y_train = Y_train, Y_test = Y_test)

"""###Feature Selection"""

featureSelected = X_train.iloc[:, chromo_df_dr[-1]].columns.tolist()
featureSelectedPaper = ['Gender_Male', 'Polyuria_Yes', 'Polydipsia_Yes', 'Genital thrush_Yes', 'visual blurring_Yes',
                        'Itching_Yes', 'Irritability_Yes', 'delayed healing_Yes', 'muscle stiffness_Yes', 'Alopecia_Yes', 'Obesity_Yes']
featureSelectedRandomFlorest = ['Polyuria_Yes',
 'Polydipsia_Yes',
 'Gender_Male',
 'Age',
 'sudden weight loss_Yes',
 'partial paresis_Yes',
 'Irritability_Yes',
 'Alopecia_Yes',
 'Polyphagia_Yes',
 'delayed healing_Yes']
featureSelectedRandomFlorest2 = ['Polyuria_Yes', 'Polydipsia_Yes', 'Age', 'Gender_Male', 'sudden weight loss_Yes', 'partial paresis_Yes']

"""##3. Models Training

###Classificadores
"""

classifiers = ['RandomForest','DecisionTree',
               'GradientBoosting',
               'ELM',       'MLP',
               'LinearSVM', 'RadialSVM(RBF)']


models = [RandomForestClassifier(n_estimators=200, random_state=0),
          DecisionTreeClassifier(random_state=0),
          GradientBoostingClassifier(random_state=0),
          ELMClassifier(n_hidden=86),
          MLPClassifier(),
          SVC(kernel='linear'),
          SVC(kernel='rbf')]

"""###Sem featureSelected"""

'''
Roda um classificador 'clf' com validação cruzada de 'folds' divisões em um
dataset cujas variáveis independentes estão em 'features' e a dependente está
em 'labels'

Return: uma lista com as acurácias para cada divisão kfod. Último elemento da
lista é a acurácia média
'''
def run_clf_with_cv(clf, kfolds, features, labels):
    kfold_acc = []

    # Iterando sobre as divisões do K-Fold
    for i, (train_index, test_index) in enumerate(kfolds):
        X_train, X_test = features.iloc[train_index], features.iloc[test_index]
        y_train, y_test = labels[train_index], labels[test_index]

        # Criando e treinando o modelo
        clf.fit(X_train, y_train)

        # Fazendo previsões nos dados de validação
        y_pred = clf.predict(X_test)

        # Calculando a precisão para esta divisão
        kfold_acc.append(accuracy_score(y_test, y_pred))
    kfold_acc.append(np.mean(kfold_acc))
    return kfold_acc

folds = 10

Acuracy_cv_df = pd.DataFrame({'Fold': [str(i) for i in range(1, folds + 2)]})

# Configurando a validação cruzada K-Fold
kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)
kfolds = list(kfold.split(X, y))

# Iterando sobre classificadores
for clf, clf_name in zip(models, classifiers):
    Acuracy_cv_df[clf_name] = run_clf_with_cv(clf, kfolds, X, y)

Acuracy_cv_df = Acuracy_cv_df.set_index('Fold').rename(index={str(folds + 1): 'Média'})
Acuracy_cv_df_display = Acuracy_cv_df.style.format('{:,.2%}'.format)
Acuracy_cv_df_display

"""###Com featureSelected"""

Xfs = X[featureSelectedRandomFlorest]
Xfs_train, Xfs_test, y_train, y_test = train_test_split(Xfs, y, test_size=0.2, random_state=42)

"""###Treinamento e validação"""

Acuracy_cv_df_display = pd.DataFrame({'Fold': [str(i) for i in range(1, folds + 2)]})
Acuracy_cv_df_display = Acuracy_cv_df_display.set_index('Fold').rename(index={str(folds + 1): 'Média'})

clf = RandomForestClassifier(n_estimators=80)
Acuracy_cv_df_display['RandomForest (FS)'] = run_clf_with_cv(clf, kfolds, Xfs, y)

clf = DecisionTreeClassifier()
kfolds = list(kfold.split(Xfs, y))
Acuracy_cv_df_display['DecisionTree (FS)'] = run_clf_with_cv(clf, kfolds, Xfs, y)

clf = GradientBoostingClassifier(random_state=0)
Acuracy_cv_df_display['GradientBoosting (FS)'] = run_clf_with_cv(clf, kfolds, Xfs, y)

clf = ELMClassifier(n_hidden=86)
Acuracy_cv_df_display['ELM (FS)'] = run_clf_with_cv(clf, kfolds, Xfs, y)

Acuracy_cv_df_display = Acuracy_cv_df_display.style.format('{:,.2%}'.format)
Acuracy_cv_df_display

clf = RandomForestClassifier(n_estimators=80)

Acuracy_cv_df['RandomForest (FS)'] = run_clf_with_cv(clf, kfolds, Xfs, y)
Acuracy_cv_df_display = Acuracy_cv_df.style.format('{:,.2%}'.format)
Acuracy_cv_df_display

clf = DecisionTreeClassifier()

kfolds = list(kfold.split(Xfs, y))

Acuracy_cv_df['DecisionTree (FS)'] = run_clf_with_cv(clf, kfolds, Xfs, y)
Acuracy_cv_df_display = Acuracy_cv_df.style.format('{:,.2%}'.format)
Acuracy_cv_df_display

clf = GradientBoostingClassifier(random_state=0)

Acuracy_cv_df['GradientBoosting (FS)'] = run_clf_with_cv(clf, kfolds, Xfs, y)
Acuracy_cv_df_display = Acuracy_cv_df.style.format('{:,.2%}'.format)
Acuracy_cv_df_display

"""###Teste"""

# Criando e treinando o modelo GradientBoosting
clf = GradientBoostingClassifier()
clf.fit(Xfs_train, y_train)

# Fazendo previsões nos dados de teste
y_pred = clf.predict(Xfs_test)
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
print(classification_report(y_test, y_pred))

# Criando e treinando o modelo RandomForest
clf = RandomForestClassifier(n_estimators=80)
clf.fit(Xfs_train, y_train)

# Fazendo previsões nos dados de teste
y_pred = clf.predict(Xfs_test)
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
print(classification_report(y_test, y_pred))

# Criando e treinando o modelo ELM
clf = ELMClassifier(n_hidden=86)
clf.fit(Xfs_train, y_train)

# Fazendo previsões nos dados de teste
y_pred = clf.predict(Xfs_test)
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
print(classification_report(y_test, y_pred))

"""###Função do teste:"""

def runClassifications():
  Score = pd.DataFrame({"Classifier":classifiers})
  acc = []
  for model in models:
    model.fit(Xfs_train, y_train)
    predictions = model.predict(Xfs_test)
    acc.append(accuracy_score(y_test, predictions))

  Score["Accuracy"] = acc
  Score.sort_values(by="Accuracy", ascending=False,inplace = True)
  Score.reset_index(drop=True, inplace=True)
  return Score

accuracies = runClassifications()
accuracies = accuracies.set_index('Classifier')
accuracies

accuracies.plot(kind='bar',
                title='Acurácias dos Modelos Treinados com Seleção de Features Baseada em Árvore',
                ylabel='Acurácia',
                xlabel='Modelos',
                figsize=(12, 4),
                ylim=[.6, 1.0],
                rot=45,
                legend=None)
plt.show()

plt.style.use('seaborn-v0_8-dark')
ConfusionMatrixDisplay.from_estimator(
    models[0], Xfs_test, y_test)
plt.xlabel('Classe Predita')
plt.ylabel('Classe Verdadeira')
plt.title('Matriz de Confusão do Modelo Random Forest')
plt.show()